{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification of comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The online store launches a new service. Now users can edit and supplement product descriptions, just like in wiki communities.\n",
    "That is, clients propose their edits and comment on the changes of others.\n",
    "The store needs a tool that will look for toxic comments and submit them for moderation.\n",
    "\n",
    "We need to create a model that will classify comments as positive and negative. Here is a dataset with markup on the toxicity of edits.\n",
    "\n",
    "Purpose: to build a model with the value of the quality metric *F1* not less than 0.75.\n",
    "\n",
    "The main stages of our project will be:\n",
    "\n",
    "* Loading and preparing data\n",
    "* Training different models\n",
    "* Evaluation of the F1-score\n",
    "* Choosing the best model, testing it, checking the model for sanity\n",
    "  \n",
    "The project is made in **Jupyter Notebook**, Notebook server version: 6.1.4. Version **Python** 3.7.8.\n",
    "The project used:\n",
    "* **re**\n",
    "* **Pandas**\n",
    "* **NumPy**\n",
    "* **scikit-learn**\n",
    "* **MatPlotLib**\n",
    "* **Spacy**\n",
    "* **NLTK**\n",
    "* **IPython**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries and modules.\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load and take a look at the dataset.\n",
    "data = pd.read_csv('toxic_comments.csv')\n",
    "display(data)\n",
    "# Examine the basic information.\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row duplicates number: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's check the dataset for missing values.\n",
    "print('Row duplicates number:', \n",
    "      data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42min 49s, sys: 2min 28s, total: 45min 18s\n",
      "Wall time: 1h 6min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's lemmatize the texts.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data['lemmatized'] = data['text'].apply(\n",
    "    lambda x: ' '.join(\n",
    "        [y.lemma_ for y in nlp(x)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get rid of the service word -PRON- of the Spacy library.\n",
    "data['lemmatized'] = data['lemmatized'].str.replace('-PRON-', '')\n",
    "# Remove extra characters using regular expressions.\n",
    "data['lemmatized'] = data['lemmatized'].apply(\n",
    "    lambda x: re.sub(\n",
    "        r'[^a-zA-Z ]', ' ', x\n",
    "    )\n",
    ")\n",
    "# Get rid of extra spaces.\n",
    "data['lemmatized'] = data['lemmatized'].apply(\n",
    "    lambda x: ' '.join(\n",
    "        x.split()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edit make under my usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour I be see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man I be really not try to edit war it be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can not make any real suggestion on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of ask when your view ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that be a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer Umm there s no actual article for pros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it look like it be actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and I really do not think you understand I com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                               lemmatized  \n",
       "0       explanation why the edit make under my usernam...  \n",
       "1       d aww he match this background colour I be see...  \n",
       "2       hey man I be really not try to edit war it be ...  \n",
       "3       More I can not make any real suggestion on imp...  \n",
       "4       you sir be my hero any chance you remember wha...  \n",
       "...                                                   ...  \n",
       "159566  and for the second time of ask when your view ...  \n",
       "159567  you should be ashamed of yourself that be a ho...  \n",
       "159568  Spitzer Umm there s no actual article for pros...  \n",
       "159569  and it look like it be actually you who put on...  \n",
       "159570  and I really do not think you understand I com...  \n",
       "\n",
       "[159571 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look at the updated dataset.\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the features_train set is (127656,)\n",
      "Size of the features_test set is (31915,)\n",
      "Size of the target_train set is (127656,)\n",
      "Size of the target_test set is (31915,)\n"
     ]
    }
   ],
   "source": [
    "# Texts processed.\n",
    "# Prepare the target feature.\n",
    "features = data['lemmatized']\n",
    "target = data['toxic']\n",
    "# Let's split the dataset into training and test sets.\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, \n",
    "    target, \n",
    "    test_size=.2, \n",
    "    random_state=12345,\n",
    "    stratify=target\n",
    ")\n",
    "# Sets are ready. Let's take a look at their sizes.\n",
    "# The original data set had 159571 objects.\n",
    "sets = [features_train, features_test, target_train, target_test]\n",
    "set_names = ['features_train', 'features_test', \n",
    "             'target_train', 'target_test']\n",
    "for name, kit in zip(set_names, sets):\n",
    "    print('Size of the', name, 'set is', kit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'The ratio of classes in the training set')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the ratio of classes in the training set.\n",
    "target_train.value_counts(normalize=True).plot(kind='bar')\n",
    "plt.title('The ratio of classes in the training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change the format of the training and test sets of features.\n",
    "corpus_train = features_train.values\n",
    "corpus_test = features_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pavelsuhih/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load English stop-words.\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "# Create a TF-IDF counter.\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the features_train_vec set is (127656, 137689)\n",
      "The size of the features_test_vec set is (31915, 137689)\n"
     ]
    }
   ],
   "source": [
    "# Learn the vocabulary and create a TF-IDF training set matrix.\n",
    "features_train_vec = count_tf_idf.fit_transform(corpus_train)\n",
    "# Create a TF-IDF test sample matrix.\n",
    "features_test_vec = count_tf_idf.transform(corpus_test)\n",
    "# Let's check the sizes of the sets.\n",
    "print('The size of the features_train_vec set is', features_train_vec.shape)\n",
    "print('The size of the features_test_vec set is', features_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the preparation phase, we did the following:\n",
    "1. The required libraries and modules are imported.\n",
    "2. Downloaded and examined the dataset. Checked for complete duplicates.\n",
    "3. Texts are lemmatized. After lemmatization, the texts were cleaned from unnecessary characters.\n",
    "4. The dataset is divided into training (80%) and test (20%) sets.\n",
    "5. An assessment of the ratio of classes in the training sample was carried out. There are significantly more non-toxic comments than toxic ones - almost 90% to 10%. It was decided to give more weight to class 1 (toxic comments) to achieve a balance.\n",
    "6. A value counter TF-IDF has been created. The TF-IDF matrices of the training and test samples have been obtained. The expected sample sizes are obtained.\n",
    "\n",
    "Let's move on to training models. We will train 2 models, choose the best one in terms of the F1 metric. Then we will do the sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are:\n",
      " {'clf__C': 1.0, 'clf__penalty': 'l1'}\n",
      "Best F1-score is 0.7530169171108821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavelsuhih/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Adjust model's hyperparameters using pipeline.\n",
    "pipe_lr = Pipeline([\n",
    "                ('clf', LogisticRegression(\n",
    "                    random_state=12345, \n",
    "                    class_weight='balanced', \n",
    "                    solver='liblinear')\n",
    "                )\n",
    "]\n",
    ")\n",
    "\n",
    "grid_params_lr = [\n",
    "    {'clf__penalty': ['l1', 'l2'],\n",
    "     'clf__C': [1.0, 0.5]\n",
    "    }\n",
    "] \n",
    "\n",
    "grid_cv_lr = GridSearchCV(estimator=pipe_lr,\n",
    "                          param_grid=grid_params_lr,\n",
    "                          scoring='f1',\n",
    "                          cv=10, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "grid_cv_lr.fit(features_train_vec, target_train)\n",
    "# Best parameters are:\n",
    "best_parameters = grid_cv_lr.best_params_\n",
    "print('Best parameters are:\\n', best_parameters)\n",
    "# Best F1-score is:\n",
    "best_cv_lr_score = grid_cv_lr.best_score_\n",
    "print('Best F1-score is', best_cv_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score is: 0.763586956521739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavelsuhih/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use the model with the best hyperparameters.\n",
    "model_lr = LogisticRegression(random_state=12345, \n",
    "                              class_weight='balanced', \n",
    "                              solver='liblinear',\n",
    "                              penalty='l1', \n",
    "                              C=1.0)\n",
    "model_lr.fit(features_train_vec, target_train)\n",
    "predicted_lr = model_lr.predict(features_test_vec)\n",
    "print('F1-score is:', f1_score(target_test, predicted_lr))\n",
    "lr_score = f1_score(target_test, predicted_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are:\n",
      " {'model__max_depth': 12, 'model__min_samples_leaf': 4, 'model__min_samples_split': 2}\n",
      "Best F1-score is 0.5929686485433358\n"
     ]
    }
   ],
   "source": [
    "# Adjust model's hyperparameters using pipeline.\n",
    "pipe_dt = Pipeline([\n",
    "                ('model', DecisionTreeClassifier(\n",
    "                    random_state=12345, \n",
    "                    class_weight='balanced'))])\n",
    "\n",
    "grid_params_dt = [\n",
    "    {'model__max_depth': [4, 8, 12],\n",
    "     'model__min_samples_split': [2, 5, 10],\n",
    "     'model__min_samples_leaf': [1, 2, 4]}\n",
    "]\n",
    "\n",
    "grid_cv_dt = GridSearchCV(estimator=pipe_dt,\n",
    "                          param_grid=grid_params_dt,\n",
    "                          scoring='f1',\n",
    "                          cv=10, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "grid_cv_dt.fit(features_train_vec, target_train)\n",
    "# Best parameters are:\n",
    "best_parameters = grid_cv_dt.best_params_\n",
    "print('Best parameters are:\\n', best_parameters)\n",
    "# Best F1-score is:\n",
    "best_cv_dt_score = grid_cv_dt.best_score_\n",
    "print('Best F1-score is', best_cv_dt_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score is: 0.5948861366360368\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree model.\n",
    "model_dt = DecisionTreeClassifier(\n",
    "    random_state=12345, \n",
    "    class_weight='balanced',\n",
    "    max_depth=12,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=4\n",
    ")\n",
    "model_dt.fit(features_train_vec, target_train)\n",
    "predicted_dt = model_dt.predict(features_test_vec)\n",
    "print('F1-score is:', f1_score(target_test, predicted_dt))\n",
    "dt_score = f1_score(target_test, predicted_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, it was planned to create and test 4 models:\n",
    "* Logistic regression\n",
    "* Gradient boosting library Light GBM\n",
    "* Decision tree\n",
    "* Random forest\n",
    "  \n",
    "Unfortunately, over and over again, when trying to train gradient boosting and random forest models, the Jupyter Notebook core died. Attempts were made to simplify the code (in order to free up RAM), to exclude different models from the project execution process. However, for some reason, the kernel would die if any of these models were used in a project. It was decided to leave only logistic regression and decision tree models.\n",
    "\n",
    "To balance classes, we used the class_weight parameter in the models, specifying the value 'balanced'.\n",
    "  \n",
    "The logistic regression model allowed us to achieve our goal. The achieved F1-score is greater than 0.75.\n",
    "Let's check the model for sanity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy model F1-score is 0.18458475540386804\n",
      "Selected model is sane\n"
     ]
    }
   ],
   "source": [
    "# Let's do the sanity check.\n",
    "dummy = DummyClassifier(random_state=12345, strategy='constant', constant=1)\n",
    "dummy.fit(features_train_vec, target_train)\n",
    "predicted_dummy = dummy.predict(features_test_vec)\n",
    "print('Dummy model F1-score is', f1_score(target_test, predicted_dummy))\n",
    "if f1_score(target_test, predicted_dummy) < lr_score:\n",
    "    print('Selected model is sane')\n",
    "else:\n",
    "    print('Selected model is insane. Improvement is required.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the data and examined it. The text has been processed. The process of text lemmatization turned out to be the most time-consuming for the computer processor. About an hour.\n",
    "We then prepared the data for training the models. Vectorized features. Determined class imbalance.\n",
    "Initially, it was planned to test 4 models, but due to technical problems, only two models were used.\n",
    "The logistic regression model showed the value of the F1-score equal to 0.766, which tells us that the task was completed. This model is sane."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1002,
    "start_time": "2022-01-24T11:26:40.323Z"
   },
   {
    "duration": 328,
    "start_time": "2022-01-24T11:26:52.303Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T11:27:50.245Z"
   },
   {
    "duration": 2606,
    "start_time": "2022-01-24T11:28:27.901Z"
   },
   {
    "duration": 9987,
    "start_time": "2022-01-24T11:28:50.741Z"
   },
   {
    "duration": 14,
    "start_time": "2022-01-24T11:29:02.245Z"
   },
   {
    "duration": 595,
    "start_time": "2022-01-24T11:46:21.324Z"
   },
   {
    "duration": 594,
    "start_time": "2022-01-24T11:46:21.921Z"
   },
   {
    "duration": 599,
    "start_time": "2022-01-24T11:50:24.457Z"
   },
   {
    "duration": 185,
    "start_time": "2022-01-24T11:51:21.034Z"
   },
   {
    "duration": 164,
    "start_time": "2022-01-24T11:51:40.841Z"
   },
   {
    "duration": 163,
    "start_time": "2022-01-24T11:51:46.770Z"
   },
   {
    "duration": 1106,
    "start_time": "2022-01-24T11:56:07.937Z"
   },
   {
    "duration": 590,
    "start_time": "2022-01-24T11:56:09.044Z"
   },
   {
    "duration": 222,
    "start_time": "2022-01-24T11:56:09.637Z"
   },
   {
    "duration": 6,
    "start_time": "2022-01-24T11:57:49.639Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T11:57:55.257Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T11:58:00.617Z"
   },
   {
    "duration": 1077,
    "start_time": "2022-01-24T12:09:56.464Z"
   },
   {
    "duration": 601,
    "start_time": "2022-01-24T12:09:57.543Z"
   },
   {
    "duration": 207,
    "start_time": "2022-01-24T12:09:58.146Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T12:09:58.354Z"
   },
   {
    "duration": 1890,
    "start_time": "2022-01-24T12:10:10.398Z"
   },
   {
    "duration": 579,
    "start_time": "2022-01-24T12:10:12.289Z"
   },
   {
    "duration": 204,
    "start_time": "2022-01-24T12:10:12.872Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:16:19.003Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:16:42.362Z"
   },
   {
    "duration": 6,
    "start_time": "2022-01-24T12:16:52.730Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:17:00.138Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:17:04.106Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T12:17:24.355Z"
   },
   {
    "duration": 1983,
    "start_time": "2022-01-24T12:17:57.098Z"
   },
   {
    "duration": 1065,
    "start_time": "2022-01-24T12:29:52.378Z"
   },
   {
    "duration": 11820,
    "start_time": "2022-01-24T12:29:53.445Z"
   },
   {
    "duration": 205,
    "start_time": "2022-01-24T12:30:05.267Z"
   },
   {
    "duration": 2423,
    "start_time": "2022-01-24T12:30:05.474Z"
   },
   {
    "duration": 1624,
    "start_time": "2022-01-24T12:35:06.580Z"
   },
   {
    "duration": 13004,
    "start_time": "2022-01-24T12:35:08.206Z"
   },
   {
    "duration": 210,
    "start_time": "2022-01-24T12:35:21.212Z"
   },
   {
    "duration": 1977,
    "start_time": "2022-01-24T12:35:21.424Z"
   },
   {
    "duration": 156,
    "start_time": "2022-01-24T12:38:39.481Z"
   },
   {
    "duration": 465,
    "start_time": "2022-01-24T12:39:40.160Z"
   },
   {
    "duration": 32,
    "start_time": "2022-01-24T14:43:33.346Z"
   },
   {
    "duration": 51,
    "start_time": "2022-01-24T14:43:42.674Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-24T14:43:50.042Z"
   },
   {
    "duration": 1974,
    "start_time": "2022-01-24T14:44:25.082Z"
   },
   {
    "duration": 1217,
    "start_time": "2022-01-24T14:44:42.710Z"
   },
   {
    "duration": 9992,
    "start_time": "2022-01-24T14:44:43.929Z"
   },
   {
    "duration": 227,
    "start_time": "2022-01-24T14:44:53.923Z"
   },
   {
    "duration": 2028,
    "start_time": "2022-01-24T14:44:54.152Z"
   },
   {
    "duration": 1080,
    "start_time": "2022-01-24T14:46:35.849Z"
   },
   {
    "duration": 9678,
    "start_time": "2022-01-24T14:46:36.931Z"
   },
   {
    "duration": 195,
    "start_time": "2022-01-24T14:46:46.611Z"
   },
   {
    "duration": 1985,
    "start_time": "2022-01-24T14:46:46.808Z"
   },
   {
    "duration": 158,
    "start_time": "2022-01-24T14:55:01.189Z"
   },
   {
    "duration": 1547,
    "start_time": "2022-01-24T15:05:43.682Z"
   },
   {
    "duration": 36,
    "start_time": "2022-01-24T15:18:57.796Z"
   },
   {
    "duration": 367,
    "start_time": "2022-01-24T15:21:15.177Z"
   },
   {
    "duration": 239,
    "start_time": "2022-01-24T15:22:05.193Z"
   },
   {
    "duration": 105338,
    "start_time": "2022-01-24T15:23:56.199Z"
   },
   {
    "duration": 1190,
    "start_time": "2022-01-24T15:26:02.171Z"
   },
   {
    "duration": 607,
    "start_time": "2022-01-24T15:26:03.363Z"
   },
   {
    "duration": 211,
    "start_time": "2022-01-24T15:26:03.972Z"
   },
   {
    "duration": 107954,
    "start_time": "2022-01-24T15:26:04.185Z"
   },
   {
    "duration": 101548,
    "start_time": "2022-01-24T15:29:02.994Z"
   },
   {
    "duration": 1290,
    "start_time": "2022-01-24T15:32:27.703Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T15:32:41.378Z"
   },
   {
    "duration": 4135,
    "start_time": "2022-01-24T15:35:05.609Z"
   },
   {
    "duration": 611,
    "start_time": "2022-01-24T15:35:09.745Z"
   },
   {
    "duration": 213,
    "start_time": "2022-01-24T15:35:10.358Z"
   },
   {
    "duration": 99980,
    "start_time": "2022-01-24T15:35:10.572Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T15:36:50.554Z"
   },
   {
    "duration": 1203,
    "start_time": "2022-01-24T15:54:35.650Z"
   },
   {
    "duration": 597,
    "start_time": "2022-01-24T15:54:36.854Z"
   },
   {
    "duration": 199,
    "start_time": "2022-01-24T15:54:37.453Z"
   },
   {
    "duration": 3133,
    "start_time": "2022-01-24T15:54:37.654Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T15:54:40.789Z"
   },
   {
    "duration": 1537,
    "start_time": "2022-01-24T16:01:18.881Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:01:23.490Z"
   },
   {
    "duration": 1282,
    "start_time": "2022-01-24T16:02:11.836Z"
   },
   {
    "duration": 607,
    "start_time": "2022-01-24T16:02:13.120Z"
   },
   {
    "duration": 204,
    "start_time": "2022-01-24T16:02:13.729Z"
   },
   {
    "duration": 2867,
    "start_time": "2022-01-24T16:02:13.934Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:02:16.803Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T16:03:19.963Z"
   },
   {
    "duration": 1627,
    "start_time": "2022-01-24T16:08:33.099Z"
   },
   {
    "duration": 631,
    "start_time": "2022-01-24T16:08:34.728Z"
   },
   {
    "duration": 205,
    "start_time": "2022-01-24T16:08:35.362Z"
   },
   {
    "duration": 2887,
    "start_time": "2022-01-24T16:08:35.569Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:08:38.458Z"
   },
   {
    "duration": 11,
    "start_time": "2022-01-24T16:08:38.463Z"
   },
   {
    "duration": 2078,
    "start_time": "2022-01-24T16:10:22.730Z"
   },
   {
    "duration": 619,
    "start_time": "2022-01-24T16:10:24.810Z"
   },
   {
    "duration": 208,
    "start_time": "2022-01-24T16:10:25.433Z"
   },
   {
    "duration": 6400,
    "start_time": "2022-01-24T16:10:25.643Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T16:10:32.045Z"
   },
   {
    "duration": 6,
    "start_time": "2022-01-24T16:10:32.050Z"
   },
   {
    "duration": 1444,
    "start_time": "2022-01-24T16:12:05.765Z"
   },
   {
    "duration": 10244,
    "start_time": "2022-01-24T16:12:07.210Z"
   },
   {
    "duration": 208,
    "start_time": "2022-01-24T16:12:17.456Z"
   },
   {
    "duration": 6358,
    "start_time": "2022-01-24T16:12:17.665Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T16:13:44.524Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:14:41.370Z"
   },
   {
    "duration": 2150,
    "start_time": "2022-01-25T10:41:59.177Z"
   },
   {
    "duration": 11379,
    "start_time": "2022-01-25T10:42:01.329Z"
   },
   {
    "duration": 203,
    "start_time": "2022-01-25T10:42:12.710Z"
   },
   {
    "duration": 2117,
    "start_time": "2022-01-25T10:42:12.915Z"
   },
   {
    "duration": 1765,
    "start_time": "2022-01-25T11:27:58.603Z"
   },
   {
    "duration": 596,
    "start_time": "2022-01-25T11:28:00.370Z"
   },
   {
    "duration": 217,
    "start_time": "2022-01-25T11:28:00.970Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T11:28:01.189Z"
   },
   {
    "duration": 2139567,
    "start_time": "2022-01-25T11:29:35.851Z"
   },
   {
    "duration": 15,
    "start_time": "2022-01-25T12:05:33.082Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T12:09:16.871Z"
   },
   {
    "duration": 1591,
    "start_time": "2022-01-25T12:12:15.261Z"
   },
   {
    "duration": 992,
    "start_time": "2022-01-25T12:12:16.854Z"
   },
   {
    "duration": 242,
    "start_time": "2022-01-25T12:12:17.849Z"
   },
   {
    "duration": 2529467,
    "start_time": "2022-01-25T12:12:18.093Z"
   },
   {
    "duration": 19,
    "start_time": "2022-01-25T12:54:27.562Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-25T12:54:27.583Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T12:59:34.738Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T13:00:01.084Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:06.852Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:10.052Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:18.204Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:28.156Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T13:00:33.003Z"
   },
   {
    "duration": 1564,
    "start_time": "2022-01-25T13:03:56.166Z"
   },
   {
    "duration": 721,
    "start_time": "2022-01-25T13:03:57.732Z"
   },
   {
    "duration": 212,
    "start_time": "2022-01-25T13:03:58.455Z"
   },
   {
    "duration": 2386102,
    "start_time": "2022-01-25T13:03:58.668Z"
   },
   {
    "duration": 13,
    "start_time": "2022-01-25T13:43:44.772Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:48:19.697Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:52:13.719Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T13:52:31.694Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:54:50.196Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:55:21.054Z"
   },
   {
    "duration": 22,
    "start_time": "2022-01-25T14:05:35.026Z"
   },
   {
    "duration": 108,
    "start_time": "2022-01-25T14:06:49.953Z"
   },
   {
    "duration": 115,
    "start_time": "2022-01-25T14:08:02.543Z"
   },
   {
    "duration": 7346,
    "start_time": "2022-01-25T14:08:07.950Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:10:47.636Z"
   },
   {
    "duration": 430,
    "start_time": "2022-01-25T14:11:48.531Z"
   },
   {
    "duration": 7970,
    "start_time": "2022-01-25T14:12:01.226Z"
   },
   {
    "duration": 392,
    "start_time": "2022-01-25T14:17:24.013Z"
   },
   {
    "duration": 307,
    "start_time": "2022-01-25T14:20:54.378Z"
   },
   {
    "duration": 293,
    "start_time": "2022-01-25T14:20:57.769Z"
   },
   {
    "duration": 300,
    "start_time": "2022-01-25T14:23:09.448Z"
   },
   {
    "duration": 16,
    "start_time": "2022-01-25T14:24:38.302Z"
   },
   {
    "duration": 1108,
    "start_time": "2022-01-25T14:38:12.679Z"
   },
   {
    "duration": 11,
    "start_time": "2022-01-25T14:38:21.896Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-25T14:38:38.231Z"
   },
   {
    "duration": 28,
    "start_time": "2022-01-25T14:38:43.568Z"
   },
   {
    "duration": 11,
    "start_time": "2022-01-25T14:38:49.920Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:39:25.557Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:39:41.919Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T14:39:47.327Z"
   },
   {
    "duration": 728,
    "start_time": "2022-01-25T14:42:04.432Z"
   },
   {
    "duration": 10,
    "start_time": "2022-01-25T14:42:15.757Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T14:42:30.253Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:38.853Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:45.580Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:50.556Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:57.165Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:43:05.372Z"
   },
   {
    "duration": 7,
    "start_time": "2022-01-25T15:41:58.824Z"
   },
   {
    "duration": 201,
    "start_time": "2022-01-25T15:53:29.751Z"
   },
   {
    "duration": 167,
    "start_time": "2022-01-25T15:53:49.550Z"
   },
   {
    "duration": 151,
    "start_time": "2022-01-25T15:53:54.397Z"
   },
   {
    "duration": 132,
    "start_time": "2022-01-25T15:54:10.774Z"
   },
   {
    "duration": 126,
    "start_time": "2022-01-25T15:54:28.461Z"
   },
   {
    "duration": 371,
    "start_time": "2022-01-25T16:00:52.050Z"
   },
   {
    "duration": 23460,
    "start_time": "2022-01-25T16:01:23.391Z"
   },
   {
    "duration": 349,
    "start_time": "2022-01-25T16:01:49.623Z"
   },
   {
    "duration": 340,
    "start_time": "2022-01-25T16:03:08.597Z"
   },
   {
    "duration": 3048,
    "start_time": "2022-01-25T16:14:57.235Z"
   },
   {
    "duration": 2407,
    "start_time": "2022-01-25T16:15:06.427Z"
   },
   {
    "duration": 2108,
    "start_time": "2022-01-25T16:31:11.868Z"
   },
   {
    "duration": 1669,
    "start_time": "2022-01-25T16:31:13.978Z"
   },
   {
    "duration": 597,
    "start_time": "2022-01-25T16:31:15.649Z"
   },
   {
    "duration": 201,
    "start_time": "2022-01-25T16:31:16.247Z"
   },
   {
    "duration": 2205806,
    "start_time": "2022-01-25T16:31:16.450Z"
   },
   {
    "duration": 455,
    "start_time": "2022-01-25T17:08:02.258Z"
   },
   {
    "duration": 1305,
    "start_time": "2022-01-25T17:08:02.717Z"
   },
   {
    "duration": 893,
    "start_time": "2022-01-25T17:08:04.024Z"
   },
   {
    "duration": 47,
    "start_time": "2022-01-25T17:08:04.919Z"
   },
   {
    "duration": 6639,
    "start_time": "2022-01-25T17:08:04.968Z"
   },
   {
    "duration": 129,
    "start_time": "2022-01-25T19:45:58.644Z"
   },
   {
    "duration": 31,
    "start_time": "2022-01-25T19:47:11.227Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:50:21.472Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:50:40.496Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:50:51.008Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:51:02.079Z"
   },
   {
    "duration": 9851,
    "start_time": "2022-01-26T11:52:29.435Z"
   },
   {
    "duration": 2933,
    "start_time": "2022-01-26T11:52:39.289Z"
   },
   {
    "duration": 762,
    "start_time": "2022-01-26T11:52:42.226Z"
   },
   {
    "duration": 291,
    "start_time": "2022-01-26T11:52:42.990Z"
   },
   {
    "duration": 3329474,
    "start_time": "2022-01-26T11:52:43.283Z"
   },
   {
    "duration": 598,
    "start_time": "2022-01-26T12:48:12.760Z"
   },
   {
    "duration": 1946,
    "start_time": "2022-01-26T12:48:13.361Z"
   },
   {
    "duration": 1077,
    "start_time": "2022-01-26T12:48:15.310Z"
   },
   {
    "duration": 51,
    "start_time": "2022-01-26T12:48:16.391Z"
   },
   {
    "duration": 268,
    "start_time": "2022-01-26T12:48:16.445Z"
   },
   {
    "duration": 213,
    "start_time": "2022-01-26T13:03:12.396Z"
   },
   {
    "duration": 44,
    "start_time": "2022-01-26T13:14:40.698Z"
   },
   {
    "duration": 217,
    "start_time": "2022-01-26T13:14:50.873Z"
   },
   {
    "duration": 8,
    "start_time": "2022-01-26T13:27:25.917Z"
   },
   {
    "duration": 8,
    "start_time": "2022-01-26T13:27:52.276Z"
   },
   {
    "duration": 7,
    "start_time": "2022-01-26T13:28:09.726Z"
   },
   {
    "duration": 3031,
    "start_time": "2022-01-26T13:29:05.836Z"
   },
   {
    "duration": 407,
    "start_time": "2022-01-26T13:29:32.610Z"
   },
   {
    "duration": 10728,
    "start_time": "2022-01-26T13:30:34.234Z"
   },
   {
    "duration": 10822,
    "start_time": "2022-01-26T13:30:55.914Z"
   },
   {
    "duration": 13841,
    "start_time": "2022-01-26T13:32:27.032Z"
   },
   {
    "duration": 291,
    "start_time": "2022-01-26T13:35:44.692Z"
   },
   {
    "duration": 3107,
    "start_time": "2022-01-26T13:36:21.540Z"
   },
   {
    "duration": 7154,
    "start_time": "2022-01-26T13:36:24.650Z"
   },
   {
    "duration": 832,
    "start_time": "2022-01-26T13:36:31.807Z"
   },
   {
    "duration": 314,
    "start_time": "2022-01-26T13:36:32.642Z"
   },
   {
    "duration": 3555303,
    "start_time": "2022-01-26T13:36:32.959Z"
   },
   {
    "duration": 579,
    "start_time": "2022-01-26T14:35:48.265Z"
   },
   {
    "duration": 2177,
    "start_time": "2022-01-26T14:35:48.848Z"
   },
   {
    "duration": 1310,
    "start_time": "2022-01-26T14:35:51.028Z"
   },
   {
    "duration": 53,
    "start_time": "2022-01-26T14:35:52.344Z"
   },
   {
    "duration": 873,
    "start_time": "2022-01-26T14:35:52.400Z"
   },
   {
    "duration": 3188,
    "start_time": "2022-01-26T14:35:53.277Z"
   },
   {
    "duration": 360,
    "start_time": "2022-01-26T14:35:56.468Z"
   },
   {
    "duration": 881,
    "start_time": "2022-01-26T14:35:56.831Z"
   },
   {
    "duration": 18444,
    "start_time": "2022-01-26T14:35:57.715Z"
   },
   {
    "duration": 13875,
    "start_time": "2022-01-26T14:43:27.554Z"
   },
   {
    "duration": 688,
    "start_time": "2022-01-26T15:13:19.906Z"
   },
   {
    "duration": 394,
    "start_time": "2022-01-26T15:25:14.542Z"
   },
   {
    "duration": 464,
    "start_time": "2022-01-26T15:30:13.396Z"
   },
   {
    "duration": 15017,
    "start_time": "2022-01-26T15:30:32.587Z"
   },
   {
    "duration": 15134,
    "start_time": "2022-01-26T15:31:01.556Z"
   },
   {
    "duration": 32478,
    "start_time": "2022-01-26T15:42:27.257Z"
   },
   {
    "duration": 864,
    "start_time": "2022-01-26T15:42:59.739Z"
   },
   {
    "duration": 319,
    "start_time": "2022-01-26T15:43:00.607Z"
   },
   {
    "duration": 3675524,
    "start_time": "2022-01-26T15:43:00.929Z"
   },
   {
    "duration": 551,
    "start_time": "2022-01-26T16:44:16.456Z"
   },
   {
    "duration": 1829,
    "start_time": "2022-01-26T16:44:17.010Z"
   },
   {
    "duration": 1090,
    "start_time": "2022-01-26T16:44:18.842Z"
   },
   {
    "duration": 492,
    "start_time": "2022-01-26T16:44:19.935Z"
   },
   {
    "duration": -79,
    "start_time": "2022-01-26T16:44:20.510Z"
   },
   {
    "duration": -89,
    "start_time": "2022-01-26T16:44:20.522Z"
   },
   {
    "duration": -89,
    "start_time": "2022-01-26T16:44:20.525Z"
   },
   {
    "duration": -87,
    "start_time": "2022-01-26T16:44:20.527Z"
   },
   {
    "duration": -87,
    "start_time": "2022-01-26T16:44:20.529Z"
   },
   {
    "duration": -86,
    "start_time": "2022-01-26T16:44:20.531Z"
   },
   {
    "duration": -86,
    "start_time": "2022-01-26T16:44:20.533Z"
   },
   {
    "duration": -85,
    "start_time": "2022-01-26T16:44:20.534Z"
   },
   {
    "duration": 106,
    "start_time": "2022-01-26T16:49:59.033Z"
   },
   {
    "duration": 118,
    "start_time": "2022-01-26T16:50:10.846Z"
   },
   {
    "duration": 232,
    "start_time": "2022-01-26T16:50:10.967Z"
   },
   {
    "duration": 2808,
    "start_time": "2022-01-26T16:50:11.202Z"
   },
   {
    "duration": 204,
    "start_time": "2022-01-26T16:50:14.013Z"
   },
   {
    "duration": 13119,
    "start_time": "2022-01-26T16:50:14.220Z"
   },
   {
    "duration": 14607,
    "start_time": "2022-01-26T16:50:27.341Z"
   },
   {
    "duration": 293,
    "start_time": "2022-01-26T16:51:43.927Z"
   },
   {
    "duration": 3778,
    "start_time": "2022-01-26T16:53:49.356Z"
   },
   {
    "duration": 814,
    "start_time": "2022-01-26T16:53:53.138Z"
   },
   {
    "duration": 319,
    "start_time": "2022-01-26T16:53:53.960Z"
   },
   {
    "duration": 3470864,
    "start_time": "2022-01-26T16:53:54.281Z"
   },
   {
    "duration": 560,
    "start_time": "2022-01-26T17:51:45.147Z"
   },
   {
    "duration": 1995,
    "start_time": "2022-01-26T17:51:45.710Z"
   },
   {
    "duration": 1166,
    "start_time": "2022-01-26T17:51:47.711Z"
   },
   {
    "duration": 125,
    "start_time": "2022-01-26T17:51:48.881Z"
   },
   {
    "duration": 267,
    "start_time": "2022-01-26T17:51:49.008Z"
   },
   {
    "duration": 2951,
    "start_time": "2022-01-26T17:51:49.277Z"
   },
   {
    "duration": 167,
    "start_time": "2022-01-26T17:51:52.233Z"
   },
   {
    "duration": 15349,
    "start_time": "2022-01-26T17:51:52.403Z"
   },
   {
    "duration": 4118,
    "start_time": "2022-01-26T17:58:43.540Z"
   },
   {
    "duration": 936,
    "start_time": "2022-01-26T17:58:47.661Z"
   },
   {
    "duration": 318,
    "start_time": "2022-01-26T17:58:48.601Z"
   },
   {
    "duration": 3663386,
    "start_time": "2022-01-26T17:58:48.922Z"
   },
   {
    "duration": 898,
    "start_time": "2022-01-26T18:59:52.311Z"
   },
   {
    "duration": 2153,
    "start_time": "2022-01-26T18:59:53.214Z"
   },
   {
    "duration": 1259,
    "start_time": "2022-01-26T18:59:55.370Z"
   },
   {
    "duration": 158,
    "start_time": "2022-01-26T18:59:56.634Z"
   },
   {
    "duration": 305,
    "start_time": "2022-01-26T18:59:56.796Z"
   },
   {
    "duration": 3124,
    "start_time": "2022-01-26T18:59:57.105Z"
   },
   {
    "duration": 238,
    "start_time": "2022-01-26T19:00:00.233Z"
   },
   {
    "duration": 19656,
    "start_time": "2022-01-26T19:00:00.474Z"
   },
   {
    "duration": 17077,
    "start_time": "2022-01-26T19:00:20.136Z"
   },
   {
    "duration": 229048,
    "start_time": "2022-01-26T19:00:37.216Z"
   },
   {
    "duration": 4574,
    "start_time": "2022-01-26T19:47:57.849Z"
   },
   {
    "duration": 3958,
    "start_time": "2022-01-26T19:48:02.425Z"
   },
   {
    "duration": 327,
    "start_time": "2022-01-26T19:48:06.387Z"
   },
   {
    "duration": 3812187,
    "start_time": "2022-01-26T19:48:06.718Z"
   },
   {
    "duration": 3892,
    "start_time": "2022-01-26T20:51:38.908Z"
   },
   {
    "duration": 36,
    "start_time": "2022-01-26T20:51:42.803Z"
   },
   {
    "duration": 142,
    "start_time": "2022-01-26T20:51:42.844Z"
   },
   {
    "duration": 279,
    "start_time": "2022-01-26T20:51:42.989Z"
   },
   {
    "duration": 3143,
    "start_time": "2022-01-26T20:51:43.271Z"
   },
   {
    "duration": 254,
    "start_time": "2022-01-26T20:51:46.417Z"
   },
   {
    "duration": 15388,
    "start_time": "2022-01-26T20:51:46.673Z"
   },
   {
    "duration": 14458,
    "start_time": "2022-01-26T20:52:02.064Z"
   },
   {
    "duration": 224073,
    "start_time": "2022-01-26T20:52:16.525Z"
   },
   {
    "duration": 53,
    "start_time": "2022-01-26T20:56:00.600Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-26T21:06:56.266Z"
   },
   {
    "duration": 34,
    "start_time": "2022-01-26T21:07:49.370Z"
   },
   {
    "duration": 35,
    "start_time": "2022-01-26T21:07:53.258Z"
   },
   {
    "duration": 47,
    "start_time": "2022-01-26T21:07:56.809Z"
   },
   {
    "duration": 43,
    "start_time": "2022-01-26T21:08:02.217Z"
   },
   {
    "duration": 39,
    "start_time": "2022-01-26T21:09:10.158Z"
   },
   {
    "duration": 40,
    "start_time": "2022-01-26T21:09:59.208Z"
   },
   {
    "duration": 38,
    "start_time": "2022-01-26T21:10:27.775Z"
   },
   {
    "duration": 39,
    "start_time": "2022-01-26T21:14:36.120Z"
   },
   {
    "duration": 41,
    "start_time": "2022-01-26T21:14:39.260Z"
   },
   {
    "duration": 429,
    "start_time": "2022-01-27T06:52:17.917Z"
   },
   {
    "duration": 2984,
    "start_time": "2022-01-27T07:01:59.882Z"
   },
   {
    "duration": 806,
    "start_time": "2022-01-27T07:02:02.868Z"
   },
   {
    "duration": 301,
    "start_time": "2022-01-27T07:02:03.677Z"
   },
   {
    "duration": 3183599,
    "start_time": "2022-01-27T07:02:03.981Z"
   },
   {
    "duration": 3290,
    "start_time": "2022-01-27T07:55:07.583Z"
   },
   {
    "duration": 22,
    "start_time": "2022-01-27T07:55:10.876Z"
   },
   {
    "duration": 126,
    "start_time": "2022-01-27T07:55:10.901Z"
   },
   {
    "duration": 238,
    "start_time": "2022-01-27T07:55:11.031Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-27T07:55:11.272Z"
   },
   {
    "duration": 340,
    "start_time": "2022-01-27T07:55:11.278Z"
   },
   {
    "duration": 10233,
    "start_time": "2022-01-27T07:55:11.620Z"
   },
   {
    "duration": 316311,
    "start_time": "2022-01-27T10:02:31.911Z"
   },
   {
    "duration": 622,
    "start_time": "2022-01-27T10:11:49.407Z"
   },
   {
    "duration": 2991,
    "start_time": "2022-01-27T10:12:06.687Z"
   },
   {
    "duration": 439,
    "start_time": "2022-01-27T10:21:37.789Z"
   },
   {
    "duration": 499,
    "start_time": "2022-01-27T10:22:40.219Z"
   },
   {
    "duration": 437,
    "start_time": "2022-01-27T10:27:48.867Z"
   },
   {
    "duration": 476,
    "start_time": "2022-01-27T10:28:24.233Z"
   },
   {
    "duration": 441,
    "start_time": "2022-01-27T10:28:40.040Z"
   },
   {
    "duration": 440,
    "start_time": "2022-01-27T10:29:26.256Z"
   },
   {
    "duration": 5190190,
    "start_time": "2022-01-27T10:31:08.950Z"
   },
   {
    "duration": 113,
    "start_time": "2022-01-27T12:04:02.888Z"
   },
   {
    "duration": 11248,
    "start_time": "2022-01-27T12:04:17.703Z"
   },
   {
    "duration": 145151,
    "start_time": "2022-01-27T12:05:21.071Z"
   },
   {
    "duration": 11224,
    "start_time": "2022-01-27T12:08:24.580Z"
   },
   {
    "duration": 31,
    "start_time": "2022-01-27T12:11:01.376Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
