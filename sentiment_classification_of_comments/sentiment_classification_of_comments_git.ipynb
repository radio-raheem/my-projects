{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение тональности комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Нам нужно создать модель, которая будет классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Цель: построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "\n",
    "Основными этапами нашего проекта станут:\n",
    "\n",
    "* Загрузка и подготовка данных\n",
    "* Обучение разных моделей\n",
    "* Оценка метрики качества F1 моделей\n",
    "* Выбор лучшей модели, её тестирование, проверка модели на вменяемость \n",
    "  \n",
    "Проект выполнен в **Jupyter Notebook**, версия сервера блокнотов: 6.1.4. Версия **Python** 3.7.8.\n",
    "В проекте использованы:\n",
    "* **re**\n",
    "* **Pandas**\n",
    "* **NumPy** \n",
    "* **scikit-learn**\n",
    "* **MatPlotLib**\n",
    "* **Spacy**\n",
    "* **NLTK**\n",
    "* **IPython**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки.\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159566</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159567</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159568</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159569</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159570</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Загрузим и взглянем на датасет.\n",
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "display(data)\n",
    "# Изучим основную информацию.\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество полных дубликатов в датасете: 0\n"
     ]
    }
   ],
   "source": [
    "# Проверим датасет на наличие полных дубликатов.\n",
    "print('Количество полных дубликатов в датасете:', \n",
    "      data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52min 33s, sys: 11.9 s, total: 52min 45s\n",
      "Wall time: 53min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Лемматизируем тексты.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data['lemmatized'] = data['text'].apply(\n",
    "    lambda x: ' '.join(\n",
    "        [y.lemma_ for y in nlp(x)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Избавимся от служебного слова -PRON- библиотеки Spacy.\n",
    "data['lemmatized'] = data['lemmatized'].str.replace('-PRON-', '')\n",
    "# Удалим лишние символы при помощи регулярных выражений.\n",
    "data['lemmatized'] = data['lemmatized'].apply(\n",
    "    lambda x: re.sub(\n",
    "        r'[^a-zA-Z ]', ' ', x\n",
    "    )\n",
    ")\n",
    "# Избавимся от лишних пробелов.\n",
    "data['lemmatized'] = data['lemmatized'].apply(\n",
    "    lambda x: ' '.join(\n",
    "        x.split()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edit make under username H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww match this background colour be seemingl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man be really not try to edit war be just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More can not make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>sir be hero any chance remember what page that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159566</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when view co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159567</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>should be ashamed of that be a horrible thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159568</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Spitzer umm there s no actual article for pros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159569</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and look like be actually who put on the speed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159570</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and really do not think understand come here a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                               lemmatized  \n",
       "0       explanation why the edit make under username H...  \n",
       "1       d aww match this background colour be seemingl...  \n",
       "2       hey man be really not try to edit war be just ...  \n",
       "3       More can not make any real suggestion on impro...  \n",
       "4       sir be hero any chance remember what page that...  \n",
       "...                                                   ...  \n",
       "159566  and for the second time of asking when view co...  \n",
       "159567  should be ashamed of that be a horrible thing ...  \n",
       "159568  Spitzer umm there s no actual article for pros...  \n",
       "159569  and look like be actually who put on the speed...  \n",
       "159570  and really do not think understand come here a...  \n",
       "\n",
       "[159571 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Взглянем на датасет после наших преобразований.\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер выборки features_train составляет (127656,)\n",
      "Размер выборки features_test составляет (31915,)\n",
      "Размер выборки target_train составляет (127656,)\n",
      "Размер выборки target_test составляет (31915,)\n"
     ]
    }
   ],
   "source": [
    "# Тексты обработаны. \n",
    "# Выделим целевой признак.\n",
    "features = data['lemmatized']\n",
    "target = data['toxic']\n",
    "# Разобьем датасет на тренировочную и тестовую выборки.\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, \n",
    "    target, \n",
    "    test_size=.2, \n",
    "    random_state=12345,\n",
    "    stratify=target\n",
    ")\n",
    "# Выборки готовы. Взглянем на их размеры. \n",
    "# Изначальный набор данных имел 159571 объект.\n",
    "sets = [features_train, features_test, target_train, target_test]\n",
    "set_names = ['features_train', 'features_test', \n",
    "             'target_train', 'target_test']\n",
    "for name, kit in zip(set_names, sets):\n",
    "    print('Размер выборки', name, 'составляет', kit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Соотношение классов в тренировочной выборке')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Оценим соотношение классов в тренировочной выборке.\n",
    "target_train.value_counts(normalize=True).plot(kind='bar')\n",
    "plt.title('Соотношение классов в тренировочной выборке')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Изменим формат тренировочного и тестового наборов признаков.\n",
    "corpus_train = features_train.values\n",
    "corpus_test = features_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Загрузим английские стоп-слова.\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "# Создадим счётчик величин TF-IDF.\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер набора features_train_vec составляет (127656, 138031)\n",
      "Размер набора features_test_vec составляет (31915, 138031)\n"
     ]
    }
   ],
   "source": [
    "# Выучим словарь и создадим TF-IDF матрицу тренировочной выборки.\n",
    "features_train_vec = count_tf_idf.fit_transform(corpus_train)\n",
    "# Создадим TF-IDF матрицу тестовой выборки.\n",
    "features_test_vec = count_tf_idf.transform(corpus_test)\n",
    "# Проверим размеры наборов.\n",
    "print('Размер набора features_train_vec составляет', features_train_vec.shape)\n",
    "print('Размер набора features_test_vec составляет', features_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этапе подготовки нами было сделано следующее:  \n",
    "1. Импортированы необходимые для выполнения проекта библиотеки.  \n",
    "2. Загружен и изучен датасет. Проверен на полные дубликаты.  \n",
    "3. Тексты лемматизированы. После лемматизации проведена очистка текстов от ненужных символов.  \n",
    "4. Датасет разбит на тренировочную (80 %) и тестовую (20 %) выборки.  \n",
    "5. Проведена оценка соотношения классов в тренировочной выборке. Нетоксичных комментариеви значительно больше, чем токсичных — почти 90 % к 10 %. Принято решение придать больший вес классу 1 (токсичные комментарии) для достижения сбалансированности.\n",
    "6. Был создан счетчик величин TF-IDF. Получены TF-IDF-матрицы тренировочной и тестовой выборок. Получены ожидаемые размеры выборок.  \n",
    "\n",
    "Перейдем к обучению моделей. Мы обучим 2 модели, выберем лучшую по показателю метрики F1. Затем проверим на вменяемость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие значения гиперпараметров:\n",
      " {'clf__C': 1.0, 'clf__penalty': 'l1'}\n",
      "Лучшая F1-мера модели логистической регрессии 0.7517491937322238\n"
     ]
    }
   ],
   "source": [
    "# Подберем гиперпараметры модели, используя pipeline.\n",
    "pipe_lr = Pipeline([\n",
    "                ('clf', LogisticRegression(\n",
    "                    random_state=12345, \n",
    "                    class_weight='balanced', \n",
    "                    solver='liblinear'))])\n",
    "\n",
    "grid_params_lr = [\n",
    "    {'clf__penalty': ['l1', 'l2'],\n",
    "     'clf__C': [1.0, 0.5]}\n",
    "] \n",
    "\n",
    "grid_cv_lr = GridSearchCV(estimator=pipe_lr,\n",
    "                          param_grid=grid_params_lr,\n",
    "                          scoring='f1',\n",
    "                          cv=10, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "grid_cv_lr.fit(features_train_vec, target_train)\n",
    "# Выявим лучшие параметры.\n",
    "best_parameters = grid_cv_lr.best_params_\n",
    "print('Лучшие значения гиперпараметров:\\n', best_parameters)\n",
    "# Выявим лучшее значение F1-меры.\n",
    "best_cv_lr_score = grid_cv_lr.best_score_\n",
    "print('Лучшая F1-мера модели логистической регрессии', best_cv_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера модели логистической регрессии составляет 0.7656909462219197\n"
     ]
    }
   ],
   "source": [
    "# Применим модель с лучшими гиперпараметрами.\n",
    "model_lr = LogisticRegression(random_state=12345, \n",
    "                              class_weight='balanced', \n",
    "                              solver='liblinear',\n",
    "                              penalty='l1', \n",
    "                              C=1.0)\n",
    "model_lr.fit(features_train_vec, target_train)\n",
    "predicted_lr = model_lr.predict(features_test_vec)\n",
    "print('F1-мера модели логистической регрессии составляет', f1_score(target_test, predicted_lr))\n",
    "lr_score = f1_score(target_test, predicted_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие значения гиперпараметров:\n",
      " {'model__max_depth': 12, 'model__min_samples_leaf': 4, 'model__min_samples_split': 10}\n",
      "Лучшая F1-мера модели дерева решений 0.5949356924707409\n"
     ]
    }
   ],
   "source": [
    "# Подберем гиперпараметры модели, используя pipeline.\n",
    "pipe_dt = Pipeline([\n",
    "                ('model', DecisionTreeClassifier(\n",
    "                    random_state=12345, \n",
    "                    class_weight='balanced'))])\n",
    "\n",
    "grid_params_dt = [\n",
    "    {'model__max_depth': [4, 8, 12],\n",
    "     'model__min_samples_split': [2, 5, 10],\n",
    "     'model__min_samples_leaf': [1, 2, 4]}\n",
    "]\n",
    "\n",
    "grid_cv_dt = GridSearchCV(estimator=pipe_dt,\n",
    "                          param_grid=grid_params_dt,\n",
    "                          scoring='f1',\n",
    "                          cv=10, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "grid_cv_dt.fit(features_train_vec, target_train)\n",
    "# Выявим лучшие параметры.\n",
    "best_parameters = grid_cv_dt.best_params_\n",
    "print('Лучшие значения гиперпараметров:\\n', best_parameters)\n",
    "# Выявим лучшее значение F1-меры.\n",
    "best_cv_dt_score = grid_cv_dt.best_score_\n",
    "print('Лучшая F1-мера модели дерева решений', best_cv_dt_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера модели дерева решений составляет 0.5950577919489837\n"
     ]
    }
   ],
   "source": [
    "# Создадим модель дерева решений.\n",
    "model_dt = DecisionTreeClassifier(\n",
    "    random_state=12345, \n",
    "    class_weight='balanced',\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4\n",
    ")\n",
    "model_dt.fit(features_train_vec, target_train)\n",
    "predicted_dt = model_dt.predict(features_test_vec)\n",
    "print('F1-мера модели дерева решений составляет', f1_score(target_test, predicted_dt))\n",
    "dt_score = f1_score(target_test, predicted_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально планировалось создать и протестировать 4 модели:  \n",
    "* Логистической регрессии\n",
    "* Градиентного бустинга библиотеки Light GBM\n",
    "* Дерева решений\n",
    "* Случайного леса  \n",
    "  \n",
    "К сожалению, раз за разом при попытке обучить модели градиентного бустинга и случайного леса ядро Jupyter Notebook умирало. Были предприняты попытки упрощения кода (в целях освобождения оперативной памяти), исключения разных моделей из процесса выполнения проекта. Тем не менее по какой-то причине ядро умирало, если хотя бы одна из этих моделей была задействована в проекте. Было принято решение оставить только модели логистической регрессии и дерева решений.  \n",
    "\n",
    "Для балансировки классов мы воспользовались параметром class_weight в моделях, указав значение 'balanced'. \n",
    "  \n",
    "Модель логистической регрессии позволила нам добиться поставленной задачи. Достигнутая F1-мера больше 0,75.  \n",
    "Проверим модель на вменяемость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера дамми-модели составляет 0.18458475540386804\n",
      "Выбранная модель вменяема\n"
     ]
    }
   ],
   "source": [
    "# Проверим модель на вменяемость.\n",
    "dummy = DummyClassifier(random_state=12345, strategy='constant', constant=1)\n",
    "dummy.fit(features_train_vec, target_train)\n",
    "predicted_dummy = dummy.predict(features_test_vec)\n",
    "print('F1-мера дамми-модели составляет', f1_score(target_test, predicted_dummy))\n",
    "if f1_score(target_test, predicted_dummy) < lr_score:\n",
    "    print('Выбранная модель вменяема')\n",
    "else:\n",
    "    print('Модель невменяема, требуется доработка')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По ходу проекта мы загрузили данные, изучили их. Была проведена обработка текста. Процесс лемматизации текста оказался самым трудоёмким для процессора компьютера. Около часа.  \n",
    "Затем мы подготовили данные к обучению моделей. Векторизовали признаки. Определили дисбаланс классов.  \n",
    "Изначально планировалось протестировать 4 модели, но ввиду технических проблем, использовались только две модели.  \n",
    "Модель логистической регрессии показала значение F1-меры равное 0,766, что говорит нам о выполнении поставленной задачи. Данная модель прошла проверку на вменяемость."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1002,
    "start_time": "2022-01-24T11:26:40.323Z"
   },
   {
    "duration": 328,
    "start_time": "2022-01-24T11:26:52.303Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T11:27:50.245Z"
   },
   {
    "duration": 2606,
    "start_time": "2022-01-24T11:28:27.901Z"
   },
   {
    "duration": 9987,
    "start_time": "2022-01-24T11:28:50.741Z"
   },
   {
    "duration": 14,
    "start_time": "2022-01-24T11:29:02.245Z"
   },
   {
    "duration": 595,
    "start_time": "2022-01-24T11:46:21.324Z"
   },
   {
    "duration": 594,
    "start_time": "2022-01-24T11:46:21.921Z"
   },
   {
    "duration": 599,
    "start_time": "2022-01-24T11:50:24.457Z"
   },
   {
    "duration": 185,
    "start_time": "2022-01-24T11:51:21.034Z"
   },
   {
    "duration": 164,
    "start_time": "2022-01-24T11:51:40.841Z"
   },
   {
    "duration": 163,
    "start_time": "2022-01-24T11:51:46.770Z"
   },
   {
    "duration": 1106,
    "start_time": "2022-01-24T11:56:07.937Z"
   },
   {
    "duration": 590,
    "start_time": "2022-01-24T11:56:09.044Z"
   },
   {
    "duration": 222,
    "start_time": "2022-01-24T11:56:09.637Z"
   },
   {
    "duration": 6,
    "start_time": "2022-01-24T11:57:49.639Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T11:57:55.257Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T11:58:00.617Z"
   },
   {
    "duration": 1077,
    "start_time": "2022-01-24T12:09:56.464Z"
   },
   {
    "duration": 601,
    "start_time": "2022-01-24T12:09:57.543Z"
   },
   {
    "duration": 207,
    "start_time": "2022-01-24T12:09:58.146Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T12:09:58.354Z"
   },
   {
    "duration": 1890,
    "start_time": "2022-01-24T12:10:10.398Z"
   },
   {
    "duration": 579,
    "start_time": "2022-01-24T12:10:12.289Z"
   },
   {
    "duration": 204,
    "start_time": "2022-01-24T12:10:12.872Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:16:19.003Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:16:42.362Z"
   },
   {
    "duration": 6,
    "start_time": "2022-01-24T12:16:52.730Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:17:00.138Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-24T12:17:04.106Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T12:17:24.355Z"
   },
   {
    "duration": 1983,
    "start_time": "2022-01-24T12:17:57.098Z"
   },
   {
    "duration": 1065,
    "start_time": "2022-01-24T12:29:52.378Z"
   },
   {
    "duration": 11820,
    "start_time": "2022-01-24T12:29:53.445Z"
   },
   {
    "duration": 205,
    "start_time": "2022-01-24T12:30:05.267Z"
   },
   {
    "duration": 2423,
    "start_time": "2022-01-24T12:30:05.474Z"
   },
   {
    "duration": 1624,
    "start_time": "2022-01-24T12:35:06.580Z"
   },
   {
    "duration": 13004,
    "start_time": "2022-01-24T12:35:08.206Z"
   },
   {
    "duration": 210,
    "start_time": "2022-01-24T12:35:21.212Z"
   },
   {
    "duration": 1977,
    "start_time": "2022-01-24T12:35:21.424Z"
   },
   {
    "duration": 156,
    "start_time": "2022-01-24T12:38:39.481Z"
   },
   {
    "duration": 465,
    "start_time": "2022-01-24T12:39:40.160Z"
   },
   {
    "duration": 32,
    "start_time": "2022-01-24T14:43:33.346Z"
   },
   {
    "duration": 51,
    "start_time": "2022-01-24T14:43:42.674Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-24T14:43:50.042Z"
   },
   {
    "duration": 1974,
    "start_time": "2022-01-24T14:44:25.082Z"
   },
   {
    "duration": 1217,
    "start_time": "2022-01-24T14:44:42.710Z"
   },
   {
    "duration": 9992,
    "start_time": "2022-01-24T14:44:43.929Z"
   },
   {
    "duration": 227,
    "start_time": "2022-01-24T14:44:53.923Z"
   },
   {
    "duration": 2028,
    "start_time": "2022-01-24T14:44:54.152Z"
   },
   {
    "duration": 1080,
    "start_time": "2022-01-24T14:46:35.849Z"
   },
   {
    "duration": 9678,
    "start_time": "2022-01-24T14:46:36.931Z"
   },
   {
    "duration": 195,
    "start_time": "2022-01-24T14:46:46.611Z"
   },
   {
    "duration": 1985,
    "start_time": "2022-01-24T14:46:46.808Z"
   },
   {
    "duration": 158,
    "start_time": "2022-01-24T14:55:01.189Z"
   },
   {
    "duration": 1547,
    "start_time": "2022-01-24T15:05:43.682Z"
   },
   {
    "duration": 36,
    "start_time": "2022-01-24T15:18:57.796Z"
   },
   {
    "duration": 367,
    "start_time": "2022-01-24T15:21:15.177Z"
   },
   {
    "duration": 239,
    "start_time": "2022-01-24T15:22:05.193Z"
   },
   {
    "duration": 105338,
    "start_time": "2022-01-24T15:23:56.199Z"
   },
   {
    "duration": 1190,
    "start_time": "2022-01-24T15:26:02.171Z"
   },
   {
    "duration": 607,
    "start_time": "2022-01-24T15:26:03.363Z"
   },
   {
    "duration": 211,
    "start_time": "2022-01-24T15:26:03.972Z"
   },
   {
    "duration": 107954,
    "start_time": "2022-01-24T15:26:04.185Z"
   },
   {
    "duration": 101548,
    "start_time": "2022-01-24T15:29:02.994Z"
   },
   {
    "duration": 1290,
    "start_time": "2022-01-24T15:32:27.703Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T15:32:41.378Z"
   },
   {
    "duration": 4135,
    "start_time": "2022-01-24T15:35:05.609Z"
   },
   {
    "duration": 611,
    "start_time": "2022-01-24T15:35:09.745Z"
   },
   {
    "duration": 213,
    "start_time": "2022-01-24T15:35:10.358Z"
   },
   {
    "duration": 99980,
    "start_time": "2022-01-24T15:35:10.572Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T15:36:50.554Z"
   },
   {
    "duration": 1203,
    "start_time": "2022-01-24T15:54:35.650Z"
   },
   {
    "duration": 597,
    "start_time": "2022-01-24T15:54:36.854Z"
   },
   {
    "duration": 199,
    "start_time": "2022-01-24T15:54:37.453Z"
   },
   {
    "duration": 3133,
    "start_time": "2022-01-24T15:54:37.654Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T15:54:40.789Z"
   },
   {
    "duration": 1537,
    "start_time": "2022-01-24T16:01:18.881Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:01:23.490Z"
   },
   {
    "duration": 1282,
    "start_time": "2022-01-24T16:02:11.836Z"
   },
   {
    "duration": 607,
    "start_time": "2022-01-24T16:02:13.120Z"
   },
   {
    "duration": 204,
    "start_time": "2022-01-24T16:02:13.729Z"
   },
   {
    "duration": 2867,
    "start_time": "2022-01-24T16:02:13.934Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:02:16.803Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T16:03:19.963Z"
   },
   {
    "duration": 1627,
    "start_time": "2022-01-24T16:08:33.099Z"
   },
   {
    "duration": 631,
    "start_time": "2022-01-24T16:08:34.728Z"
   },
   {
    "duration": 205,
    "start_time": "2022-01-24T16:08:35.362Z"
   },
   {
    "duration": 2887,
    "start_time": "2022-01-24T16:08:35.569Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:08:38.458Z"
   },
   {
    "duration": 11,
    "start_time": "2022-01-24T16:08:38.463Z"
   },
   {
    "duration": 2078,
    "start_time": "2022-01-24T16:10:22.730Z"
   },
   {
    "duration": 619,
    "start_time": "2022-01-24T16:10:24.810Z"
   },
   {
    "duration": 208,
    "start_time": "2022-01-24T16:10:25.433Z"
   },
   {
    "duration": 6400,
    "start_time": "2022-01-24T16:10:25.643Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T16:10:32.045Z"
   },
   {
    "duration": 6,
    "start_time": "2022-01-24T16:10:32.050Z"
   },
   {
    "duration": 1444,
    "start_time": "2022-01-24T16:12:05.765Z"
   },
   {
    "duration": 10244,
    "start_time": "2022-01-24T16:12:07.210Z"
   },
   {
    "duration": 208,
    "start_time": "2022-01-24T16:12:17.456Z"
   },
   {
    "duration": 6358,
    "start_time": "2022-01-24T16:12:17.665Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-24T16:13:44.524Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-24T16:14:41.370Z"
   },
   {
    "duration": 2150,
    "start_time": "2022-01-25T10:41:59.177Z"
   },
   {
    "duration": 11379,
    "start_time": "2022-01-25T10:42:01.329Z"
   },
   {
    "duration": 203,
    "start_time": "2022-01-25T10:42:12.710Z"
   },
   {
    "duration": 2117,
    "start_time": "2022-01-25T10:42:12.915Z"
   },
   {
    "duration": 1765,
    "start_time": "2022-01-25T11:27:58.603Z"
   },
   {
    "duration": 596,
    "start_time": "2022-01-25T11:28:00.370Z"
   },
   {
    "duration": 217,
    "start_time": "2022-01-25T11:28:00.970Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T11:28:01.189Z"
   },
   {
    "duration": 2139567,
    "start_time": "2022-01-25T11:29:35.851Z"
   },
   {
    "duration": 15,
    "start_time": "2022-01-25T12:05:33.082Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T12:09:16.871Z"
   },
   {
    "duration": 1591,
    "start_time": "2022-01-25T12:12:15.261Z"
   },
   {
    "duration": 992,
    "start_time": "2022-01-25T12:12:16.854Z"
   },
   {
    "duration": 242,
    "start_time": "2022-01-25T12:12:17.849Z"
   },
   {
    "duration": 2529467,
    "start_time": "2022-01-25T12:12:18.093Z"
   },
   {
    "duration": 19,
    "start_time": "2022-01-25T12:54:27.562Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-25T12:54:27.583Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T12:59:34.738Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T13:00:01.084Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:06.852Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:10.052Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:18.204Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:00:28.156Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T13:00:33.003Z"
   },
   {
    "duration": 1564,
    "start_time": "2022-01-25T13:03:56.166Z"
   },
   {
    "duration": 721,
    "start_time": "2022-01-25T13:03:57.732Z"
   },
   {
    "duration": 212,
    "start_time": "2022-01-25T13:03:58.455Z"
   },
   {
    "duration": 2386102,
    "start_time": "2022-01-25T13:03:58.668Z"
   },
   {
    "duration": 13,
    "start_time": "2022-01-25T13:43:44.772Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:48:19.697Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:52:13.719Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T13:52:31.694Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:54:50.196Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T13:55:21.054Z"
   },
   {
    "duration": 22,
    "start_time": "2022-01-25T14:05:35.026Z"
   },
   {
    "duration": 108,
    "start_time": "2022-01-25T14:06:49.953Z"
   },
   {
    "duration": 115,
    "start_time": "2022-01-25T14:08:02.543Z"
   },
   {
    "duration": 7346,
    "start_time": "2022-01-25T14:08:07.950Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:10:47.636Z"
   },
   {
    "duration": 430,
    "start_time": "2022-01-25T14:11:48.531Z"
   },
   {
    "duration": 7970,
    "start_time": "2022-01-25T14:12:01.226Z"
   },
   {
    "duration": 392,
    "start_time": "2022-01-25T14:17:24.013Z"
   },
   {
    "duration": 307,
    "start_time": "2022-01-25T14:20:54.378Z"
   },
   {
    "duration": 293,
    "start_time": "2022-01-25T14:20:57.769Z"
   },
   {
    "duration": 300,
    "start_time": "2022-01-25T14:23:09.448Z"
   },
   {
    "duration": 16,
    "start_time": "2022-01-25T14:24:38.302Z"
   },
   {
    "duration": 1108,
    "start_time": "2022-01-25T14:38:12.679Z"
   },
   {
    "duration": 11,
    "start_time": "2022-01-25T14:38:21.896Z"
   },
   {
    "duration": 5,
    "start_time": "2022-01-25T14:38:38.231Z"
   },
   {
    "duration": 28,
    "start_time": "2022-01-25T14:38:43.568Z"
   },
   {
    "duration": 11,
    "start_time": "2022-01-25T14:38:49.920Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:39:25.557Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:39:41.919Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T14:39:47.327Z"
   },
   {
    "duration": 728,
    "start_time": "2022-01-25T14:42:04.432Z"
   },
   {
    "duration": 10,
    "start_time": "2022-01-25T14:42:15.757Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-25T14:42:30.253Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:38.853Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:45.580Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:50.556Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:42:57.165Z"
   },
   {
    "duration": 4,
    "start_time": "2022-01-25T14:43:05.372Z"
   },
   {
    "duration": 7,
    "start_time": "2022-01-25T15:41:58.824Z"
   },
   {
    "duration": 201,
    "start_time": "2022-01-25T15:53:29.751Z"
   },
   {
    "duration": 167,
    "start_time": "2022-01-25T15:53:49.550Z"
   },
   {
    "duration": 151,
    "start_time": "2022-01-25T15:53:54.397Z"
   },
   {
    "duration": 132,
    "start_time": "2022-01-25T15:54:10.774Z"
   },
   {
    "duration": 126,
    "start_time": "2022-01-25T15:54:28.461Z"
   },
   {
    "duration": 371,
    "start_time": "2022-01-25T16:00:52.050Z"
   },
   {
    "duration": 23460,
    "start_time": "2022-01-25T16:01:23.391Z"
   },
   {
    "duration": 349,
    "start_time": "2022-01-25T16:01:49.623Z"
   },
   {
    "duration": 340,
    "start_time": "2022-01-25T16:03:08.597Z"
   },
   {
    "duration": 3048,
    "start_time": "2022-01-25T16:14:57.235Z"
   },
   {
    "duration": 2407,
    "start_time": "2022-01-25T16:15:06.427Z"
   },
   {
    "duration": 2108,
    "start_time": "2022-01-25T16:31:11.868Z"
   },
   {
    "duration": 1669,
    "start_time": "2022-01-25T16:31:13.978Z"
   },
   {
    "duration": 597,
    "start_time": "2022-01-25T16:31:15.649Z"
   },
   {
    "duration": 201,
    "start_time": "2022-01-25T16:31:16.247Z"
   },
   {
    "duration": 2205806,
    "start_time": "2022-01-25T16:31:16.450Z"
   },
   {
    "duration": 455,
    "start_time": "2022-01-25T17:08:02.258Z"
   },
   {
    "duration": 1305,
    "start_time": "2022-01-25T17:08:02.717Z"
   },
   {
    "duration": 893,
    "start_time": "2022-01-25T17:08:04.024Z"
   },
   {
    "duration": 47,
    "start_time": "2022-01-25T17:08:04.919Z"
   },
   {
    "duration": 6639,
    "start_time": "2022-01-25T17:08:04.968Z"
   },
   {
    "duration": 129,
    "start_time": "2022-01-25T19:45:58.644Z"
   },
   {
    "duration": 31,
    "start_time": "2022-01-25T19:47:11.227Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:50:21.472Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:50:40.496Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:50:51.008Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-25T19:51:02.079Z"
   },
   {
    "duration": 9851,
    "start_time": "2022-01-26T11:52:29.435Z"
   },
   {
    "duration": 2933,
    "start_time": "2022-01-26T11:52:39.289Z"
   },
   {
    "duration": 762,
    "start_time": "2022-01-26T11:52:42.226Z"
   },
   {
    "duration": 291,
    "start_time": "2022-01-26T11:52:42.990Z"
   },
   {
    "duration": 3329474,
    "start_time": "2022-01-26T11:52:43.283Z"
   },
   {
    "duration": 598,
    "start_time": "2022-01-26T12:48:12.760Z"
   },
   {
    "duration": 1946,
    "start_time": "2022-01-26T12:48:13.361Z"
   },
   {
    "duration": 1077,
    "start_time": "2022-01-26T12:48:15.310Z"
   },
   {
    "duration": 51,
    "start_time": "2022-01-26T12:48:16.391Z"
   },
   {
    "duration": 268,
    "start_time": "2022-01-26T12:48:16.445Z"
   },
   {
    "duration": 213,
    "start_time": "2022-01-26T13:03:12.396Z"
   },
   {
    "duration": 44,
    "start_time": "2022-01-26T13:14:40.698Z"
   },
   {
    "duration": 217,
    "start_time": "2022-01-26T13:14:50.873Z"
   },
   {
    "duration": 8,
    "start_time": "2022-01-26T13:27:25.917Z"
   },
   {
    "duration": 8,
    "start_time": "2022-01-26T13:27:52.276Z"
   },
   {
    "duration": 7,
    "start_time": "2022-01-26T13:28:09.726Z"
   },
   {
    "duration": 3031,
    "start_time": "2022-01-26T13:29:05.836Z"
   },
   {
    "duration": 407,
    "start_time": "2022-01-26T13:29:32.610Z"
   },
   {
    "duration": 10728,
    "start_time": "2022-01-26T13:30:34.234Z"
   },
   {
    "duration": 10822,
    "start_time": "2022-01-26T13:30:55.914Z"
   },
   {
    "duration": 13841,
    "start_time": "2022-01-26T13:32:27.032Z"
   },
   {
    "duration": 291,
    "start_time": "2022-01-26T13:35:44.692Z"
   },
   {
    "duration": 3107,
    "start_time": "2022-01-26T13:36:21.540Z"
   },
   {
    "duration": 7154,
    "start_time": "2022-01-26T13:36:24.650Z"
   },
   {
    "duration": 832,
    "start_time": "2022-01-26T13:36:31.807Z"
   },
   {
    "duration": 314,
    "start_time": "2022-01-26T13:36:32.642Z"
   },
   {
    "duration": 3555303,
    "start_time": "2022-01-26T13:36:32.959Z"
   },
   {
    "duration": 579,
    "start_time": "2022-01-26T14:35:48.265Z"
   },
   {
    "duration": 2177,
    "start_time": "2022-01-26T14:35:48.848Z"
   },
   {
    "duration": 1310,
    "start_time": "2022-01-26T14:35:51.028Z"
   },
   {
    "duration": 53,
    "start_time": "2022-01-26T14:35:52.344Z"
   },
   {
    "duration": 873,
    "start_time": "2022-01-26T14:35:52.400Z"
   },
   {
    "duration": 3188,
    "start_time": "2022-01-26T14:35:53.277Z"
   },
   {
    "duration": 360,
    "start_time": "2022-01-26T14:35:56.468Z"
   },
   {
    "duration": 881,
    "start_time": "2022-01-26T14:35:56.831Z"
   },
   {
    "duration": 18444,
    "start_time": "2022-01-26T14:35:57.715Z"
   },
   {
    "duration": 13875,
    "start_time": "2022-01-26T14:43:27.554Z"
   },
   {
    "duration": 688,
    "start_time": "2022-01-26T15:13:19.906Z"
   },
   {
    "duration": 394,
    "start_time": "2022-01-26T15:25:14.542Z"
   },
   {
    "duration": 464,
    "start_time": "2022-01-26T15:30:13.396Z"
   },
   {
    "duration": 15017,
    "start_time": "2022-01-26T15:30:32.587Z"
   },
   {
    "duration": 15134,
    "start_time": "2022-01-26T15:31:01.556Z"
   },
   {
    "duration": 32478,
    "start_time": "2022-01-26T15:42:27.257Z"
   },
   {
    "duration": 864,
    "start_time": "2022-01-26T15:42:59.739Z"
   },
   {
    "duration": 319,
    "start_time": "2022-01-26T15:43:00.607Z"
   },
   {
    "duration": 3675524,
    "start_time": "2022-01-26T15:43:00.929Z"
   },
   {
    "duration": 551,
    "start_time": "2022-01-26T16:44:16.456Z"
   },
   {
    "duration": 1829,
    "start_time": "2022-01-26T16:44:17.010Z"
   },
   {
    "duration": 1090,
    "start_time": "2022-01-26T16:44:18.842Z"
   },
   {
    "duration": 492,
    "start_time": "2022-01-26T16:44:19.935Z"
   },
   {
    "duration": -79,
    "start_time": "2022-01-26T16:44:20.510Z"
   },
   {
    "duration": -89,
    "start_time": "2022-01-26T16:44:20.522Z"
   },
   {
    "duration": -89,
    "start_time": "2022-01-26T16:44:20.525Z"
   },
   {
    "duration": -87,
    "start_time": "2022-01-26T16:44:20.527Z"
   },
   {
    "duration": -87,
    "start_time": "2022-01-26T16:44:20.529Z"
   },
   {
    "duration": -86,
    "start_time": "2022-01-26T16:44:20.531Z"
   },
   {
    "duration": -86,
    "start_time": "2022-01-26T16:44:20.533Z"
   },
   {
    "duration": -85,
    "start_time": "2022-01-26T16:44:20.534Z"
   },
   {
    "duration": 106,
    "start_time": "2022-01-26T16:49:59.033Z"
   },
   {
    "duration": 118,
    "start_time": "2022-01-26T16:50:10.846Z"
   },
   {
    "duration": 232,
    "start_time": "2022-01-26T16:50:10.967Z"
   },
   {
    "duration": 2808,
    "start_time": "2022-01-26T16:50:11.202Z"
   },
   {
    "duration": 204,
    "start_time": "2022-01-26T16:50:14.013Z"
   },
   {
    "duration": 13119,
    "start_time": "2022-01-26T16:50:14.220Z"
   },
   {
    "duration": 14607,
    "start_time": "2022-01-26T16:50:27.341Z"
   },
   {
    "duration": 293,
    "start_time": "2022-01-26T16:51:43.927Z"
   },
   {
    "duration": 3778,
    "start_time": "2022-01-26T16:53:49.356Z"
   },
   {
    "duration": 814,
    "start_time": "2022-01-26T16:53:53.138Z"
   },
   {
    "duration": 319,
    "start_time": "2022-01-26T16:53:53.960Z"
   },
   {
    "duration": 3470864,
    "start_time": "2022-01-26T16:53:54.281Z"
   },
   {
    "duration": 560,
    "start_time": "2022-01-26T17:51:45.147Z"
   },
   {
    "duration": 1995,
    "start_time": "2022-01-26T17:51:45.710Z"
   },
   {
    "duration": 1166,
    "start_time": "2022-01-26T17:51:47.711Z"
   },
   {
    "duration": 125,
    "start_time": "2022-01-26T17:51:48.881Z"
   },
   {
    "duration": 267,
    "start_time": "2022-01-26T17:51:49.008Z"
   },
   {
    "duration": 2951,
    "start_time": "2022-01-26T17:51:49.277Z"
   },
   {
    "duration": 167,
    "start_time": "2022-01-26T17:51:52.233Z"
   },
   {
    "duration": 15349,
    "start_time": "2022-01-26T17:51:52.403Z"
   },
   {
    "duration": 4118,
    "start_time": "2022-01-26T17:58:43.540Z"
   },
   {
    "duration": 936,
    "start_time": "2022-01-26T17:58:47.661Z"
   },
   {
    "duration": 318,
    "start_time": "2022-01-26T17:58:48.601Z"
   },
   {
    "duration": 3663386,
    "start_time": "2022-01-26T17:58:48.922Z"
   },
   {
    "duration": 898,
    "start_time": "2022-01-26T18:59:52.311Z"
   },
   {
    "duration": 2153,
    "start_time": "2022-01-26T18:59:53.214Z"
   },
   {
    "duration": 1259,
    "start_time": "2022-01-26T18:59:55.370Z"
   },
   {
    "duration": 158,
    "start_time": "2022-01-26T18:59:56.634Z"
   },
   {
    "duration": 305,
    "start_time": "2022-01-26T18:59:56.796Z"
   },
   {
    "duration": 3124,
    "start_time": "2022-01-26T18:59:57.105Z"
   },
   {
    "duration": 238,
    "start_time": "2022-01-26T19:00:00.233Z"
   },
   {
    "duration": 19656,
    "start_time": "2022-01-26T19:00:00.474Z"
   },
   {
    "duration": 17077,
    "start_time": "2022-01-26T19:00:20.136Z"
   },
   {
    "duration": 229048,
    "start_time": "2022-01-26T19:00:37.216Z"
   },
   {
    "duration": 4574,
    "start_time": "2022-01-26T19:47:57.849Z"
   },
   {
    "duration": 3958,
    "start_time": "2022-01-26T19:48:02.425Z"
   },
   {
    "duration": 327,
    "start_time": "2022-01-26T19:48:06.387Z"
   },
   {
    "duration": 3812187,
    "start_time": "2022-01-26T19:48:06.718Z"
   },
   {
    "duration": 3892,
    "start_time": "2022-01-26T20:51:38.908Z"
   },
   {
    "duration": 36,
    "start_time": "2022-01-26T20:51:42.803Z"
   },
   {
    "duration": 142,
    "start_time": "2022-01-26T20:51:42.844Z"
   },
   {
    "duration": 279,
    "start_time": "2022-01-26T20:51:42.989Z"
   },
   {
    "duration": 3143,
    "start_time": "2022-01-26T20:51:43.271Z"
   },
   {
    "duration": 254,
    "start_time": "2022-01-26T20:51:46.417Z"
   },
   {
    "duration": 15388,
    "start_time": "2022-01-26T20:51:46.673Z"
   },
   {
    "duration": 14458,
    "start_time": "2022-01-26T20:52:02.064Z"
   },
   {
    "duration": 224073,
    "start_time": "2022-01-26T20:52:16.525Z"
   },
   {
    "duration": 53,
    "start_time": "2022-01-26T20:56:00.600Z"
   },
   {
    "duration": 33,
    "start_time": "2022-01-26T21:06:56.266Z"
   },
   {
    "duration": 34,
    "start_time": "2022-01-26T21:07:49.370Z"
   },
   {
    "duration": 35,
    "start_time": "2022-01-26T21:07:53.258Z"
   },
   {
    "duration": 47,
    "start_time": "2022-01-26T21:07:56.809Z"
   },
   {
    "duration": 43,
    "start_time": "2022-01-26T21:08:02.217Z"
   },
   {
    "duration": 39,
    "start_time": "2022-01-26T21:09:10.158Z"
   },
   {
    "duration": 40,
    "start_time": "2022-01-26T21:09:59.208Z"
   },
   {
    "duration": 38,
    "start_time": "2022-01-26T21:10:27.775Z"
   },
   {
    "duration": 39,
    "start_time": "2022-01-26T21:14:36.120Z"
   },
   {
    "duration": 41,
    "start_time": "2022-01-26T21:14:39.260Z"
   },
   {
    "duration": 429,
    "start_time": "2022-01-27T06:52:17.917Z"
   },
   {
    "duration": 2984,
    "start_time": "2022-01-27T07:01:59.882Z"
   },
   {
    "duration": 806,
    "start_time": "2022-01-27T07:02:02.868Z"
   },
   {
    "duration": 301,
    "start_time": "2022-01-27T07:02:03.677Z"
   },
   {
    "duration": 3183599,
    "start_time": "2022-01-27T07:02:03.981Z"
   },
   {
    "duration": 3290,
    "start_time": "2022-01-27T07:55:07.583Z"
   },
   {
    "duration": 22,
    "start_time": "2022-01-27T07:55:10.876Z"
   },
   {
    "duration": 126,
    "start_time": "2022-01-27T07:55:10.901Z"
   },
   {
    "duration": 238,
    "start_time": "2022-01-27T07:55:11.031Z"
   },
   {
    "duration": 3,
    "start_time": "2022-01-27T07:55:11.272Z"
   },
   {
    "duration": 340,
    "start_time": "2022-01-27T07:55:11.278Z"
   },
   {
    "duration": 10233,
    "start_time": "2022-01-27T07:55:11.620Z"
   },
   {
    "duration": 316311,
    "start_time": "2022-01-27T10:02:31.911Z"
   },
   {
    "duration": 622,
    "start_time": "2022-01-27T10:11:49.407Z"
   },
   {
    "duration": 2991,
    "start_time": "2022-01-27T10:12:06.687Z"
   },
   {
    "duration": 439,
    "start_time": "2022-01-27T10:21:37.789Z"
   },
   {
    "duration": 499,
    "start_time": "2022-01-27T10:22:40.219Z"
   },
   {
    "duration": 437,
    "start_time": "2022-01-27T10:27:48.867Z"
   },
   {
    "duration": 476,
    "start_time": "2022-01-27T10:28:24.233Z"
   },
   {
    "duration": 441,
    "start_time": "2022-01-27T10:28:40.040Z"
   },
   {
    "duration": 440,
    "start_time": "2022-01-27T10:29:26.256Z"
   },
   {
    "duration": 5190190,
    "start_time": "2022-01-27T10:31:08.950Z"
   },
   {
    "duration": 113,
    "start_time": "2022-01-27T12:04:02.888Z"
   },
   {
    "duration": 11248,
    "start_time": "2022-01-27T12:04:17.703Z"
   },
   {
    "duration": 145151,
    "start_time": "2022-01-27T12:05:21.071Z"
   },
   {
    "duration": 11224,
    "start_time": "2022-01-27T12:08:24.580Z"
   },
   {
    "duration": 31,
    "start_time": "2022-01-27T12:11:01.376Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
